% -*- latex -*-

\documentclass{article}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{varioref}
\usepackage{fancyvrb}
\usepackage{ifthen}
\usepackage{cite}
\usepackage{xspace}
\usepackage{hyperref}

\usepackage{color}
\definecolor{yellow}{rgb}{1,1,0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{ltcyan}{rgb}{.75,1,1}
\definecolor{blue}{rgb}{0,0,1}
\definecolor{red}{rgb}{1,0,0}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}

% Cite commands I use to abstract away the different ways to reference an
% entry in the bibliography (superscripts, numbers, dates, or author
% abbreviations).  \scite is a short cite that is used immediately after
% when the authors are mentioned.  \lcite is a full citation that is used
% anywhere.  Both should be used right next to the text being cited without
% any spacing.
\newcommand*{\lcite}[1]{~\cite{#1}}
\newcommand*{\scite}[1]{~\cite{#1}}

\newcommand*{\keyterm}[1]{\emph{#1}}

\newcommand{\etal}{et al.}

\newcommand{\fix}[1]{{\color{red}\textbf{\textsc{[#1]}}}}

% Avoid putting figures on their own page.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.5}

% Make sure this is big enough so that only big figures end up on their own
% page but small enough so that if a figure does have to be on its own
% page, it won't push everything to the bottom because it's not big enough
% to have its own page.
\renewcommand{\floatpagefraction}{.75}

\title{Modern Visualization Pipelines}

\author{Kenneth Moreland}

\date{}

\sloppy

\begin{document}

\maketitle


\section*{Abstract}

The most common abstraction used by visualization libraries and
applications today is what is known as the visualization pipeline.  The
visualization pipeline provides a mechanism to encapsulate algorithms and
then couple them together in a variety of ways.  The visualization pipeline
has been in existence for over twenty years, and over this time many
variations and improvements have been proposed.  This paper provides a
literature review of the most prevalent features of visualization pipelines
and some of the most recent research directions.


\section{Introduction}
\label{sec:Introduction}

The field of scientific visualization was launched with the 1987 National
Science Foundation Visualization in Scientific Computing workshop
report\lcite{ViSC1987}, and some of the first proposed frameworks used a
\keyterm{visualization pipeline} for managing the ingestion,
transformation, display, and recording of
data\lcite{Haeberli1988,Lucas1992}.  The combination of simplicity and
power makes the visualization pipeline still the most prevalent metaphor
encountered today.

The visualization pipeline provides the key
structure in many visualization development systems such as the
Visualization Toolkit (VTK)\lcite{VTK}, SCIRun\lcite{SCIRun}, the
Application Visualization System (AVS)\lcite{AVS}, OpenDX\lcite{OpenDX},
and Iris Explorer\lcite{IRISExplorer}.  Visualization applications like
ParaView\lcite{ParaView}, VisTrails\lcite{VisTrails}, and
MayaVi\lcite{MayaVi} allow end users to build visualization pipelines with
graphical user interface representations.  The visualization pipeline is
also used internally in a number of other applications including
VisIt\lcite{VisIt}, VolView\lcite{VolView}, OsiriX\lcite{OsiriX}, 3D
Slicer\lcite{3DSlicer}, and BioImageXD\lcite{BioImageXD}.

In this paper we review the visualization pipeline.  We begin with a basic
description of what the visualization pipeline is and then move to
advancements introduced over the years and current research.


\section{Basic Visualization Pipelines}
\label{sec:BasicVisualizationPipelines}

A visualization pipeline encapsulates an algorithm in one of three types of
objects: \keyterm{sources}, \keyterm{filters}, and \keyterm{sinks}.  A
source object produces data that it makes available through an
\keyterm{output}.  File readers and synthetic data generators are typical
source objects.  A sink object accepts data through an \keyterm{input} and
performs an operation with no further result (as far as the pipeline is
concerned).  Typical sinks are file writers and rendering components that
provide images to a user interface.  A filter object has at least one input
from which it transform data and provides results through at least one
output.

The intention is to encapsulate algorithms in interchangeable source,
filter, and sink components with generic connection ports (inputs and
outputs).  An output from one component can be connected to the input from
another component such that the results of one algorithm become the inputs
to another algorithm.  These connected components form a
\keyterm{pipeline}.  Figure~\ref{fig:SimplePipeline} demonstrates a simple
but common pipeline featuring a file reader (source), an isosurface
generator\lcite{Lorensen1987} (filter), and an image renderer (sink).

\begin{figure}[htbp]
  \centering
  \includegraphics{images/SimplePipeline}
  \caption{A simple visualization pipeline.}
  \label{fig:SimplePipeline}
\end{figure}

Pipeline components are highly interchangeable.  Any two components can be
connected so long as the data in the output is compatible with the expected
data of the downstream input.  Pipelines can be arbitrarily deep.
Pipelines can also branch.  A \keyterm{fan out} occurs when the output of
one component is connected to the inputs of multiple other components.  A
\keyterm{fan in} occurs when a component accepts multiple inputs that can
come from separate component outputs.  Figure~\ref{fig:BranchingPipeline}
demonstrates a pipeline with branching (adapted from the ParaView
tutorial\lcite{ParaViewTutorial}).

\begin{figure}[htbp]
  \centering
  \begin{tabular}{cc}
    \includegraphics{images/BranchingPipeline} &
    \includegraphics[height=1.7in]{images/BranchingPipelineResult}
  \end{tabular}
  \caption{A visualization pipeline with branching.  The final
    visualization is shown to the right.}
  \label{fig:BranchingPipeline}
\end{figure}

For more information on using visualization pipelines and the components
they typically contain, consult the documentation for one of the numerous
libraries or applications using a visualization
pipeline\lcite{VTK,ParaView,ParaViewTutorial,SCIRunUserGuide,IRISExplorerUsersGuide,VisTrailsDocumentation}.
\fix{Add OpenDX.  Site was down.}


\section{Execution Management}
\label{sec:ExecutionManagement}

The topology of a pipeline dictates the flow of data and places constraints
on the order in which components can be executed, but it does not determine
how or when components get executed.  Visualization pipeline systems can
vary significantly in how they manage execution.

\subsection{Execution Drivers}
\label{sec:ExecutionDrivers}

The visualization pipeline represents a static network of operations
through which data flows.  Typical usage entails first establishing the
visualization pipeline and then executing the pipeline on one or more data
collections.  Consequently, the behavior of when components get executed is
a primary feature of visualization pipeline systems.  Visualization
pipelines generally fall under two execution systems: event driven and
demand driven.

An \keyterm{even-driven} pipeline launches execution as data becomes
available in sources.  When new data becomes available in a source, that
source component must be alerted.  When sources produce data, they push it
to the downstream components and trigger an event to execute them.  Those
downstream components in turn may produce their own data to push to the
next component.  Because the method of the event-driven pipeline is to push
data to the downstream components, this method is also known as the
\keyterm{push model}.  The event-driven method of execution is useful when
applying a visualization pipeline to data that is expected to change over
time.

A \keyterm{demand-driven} pipeline launches execution in response to
requests for data.  Execution is initiated at the bottom of the pipeline in
a sink.  The sink's upstream components satisfy this request by first
requesting data from their upstream components, and so on up to the
sources.  Once execution reaches a source, it produces data and returns
execution back to its downstream components.  The execution eventually
unrolls back to the originating sink.  Because the method of the
demand-driven pipeline is to pull data from the upstream components, this
method is also known as the \keyterm{pull model}.  The demand-driven method
of execution is useful when using a visualization pipeline to provide data
to an end system.  For example, the visualization could respond to render
requests to update a GUI.

\subsection{Caching Intermediate Values}
\label{sec:Caching}

Caching, which saves component execution outputs, is an important feature
for both execution methods.  In the case of the event-driven pipeline, a
component may execute only when data from all inputs is pushed to it.
Thus, the execution must know when to cache the data and where to retrieve
it when the rest of the data is later pushed.

In the case of the demand-driven pipeline, a component with fan out could
receive pull requests from multiple downstream components during the same
original sink request.  Rather than execute multiple times, the component
can first check to see if the previously computed result is still valid and
return that if possible.

Although caching all the intermediate values in a pipeline can remove
redundant computation, it also clearly requires more storage.  Thus,
managing the caching often involves a trade-off between speed and memory.
The cost of caching can be mitigated by favoring shallow copies of data
from a component's inputs to its outputs.

\subsection{Centralized vs. Distributed Control}
\label{sec:CentralizedDistributed}

The control mechanism for a visualization pipeline can be either
centralized or distributed.  A \keyterm{centralized control} has a single
unit managing the execution of all components in the pipeline.  The
centralized control has links to all components, understands their
connections, and initiates all execution in the pipeline.

A \keyterm{distributed control} has a separate unit for each component in
the pipeline.  The distributed control unit nominally knows only about a
single component and its inputs and outputs.  The distributed control unit
can initiate execution on only its own component and must send messages to
propagate execution elsewhere.

Centralized control is advantageous in that it can perform a more
thorough analysis of the pipeline's network to more finely control the
execution.  Such knowledge can be useful in making decisions about caching
(described in Section~\ref{sec:Caching}) and load balancing for parallel
execution (described in Section~\ref{sec:ParallelExecution}).  However, the
implementation of a centralized control is more complex because of the
larger management task.  Distributed control, in contrast, has more limited
knowledge of the pipeline, but tends to be simpler to implement and
manage.

\subsection{Interchangeable Executive}
\label{sec:InterchangeableExecutive}

Many visualization pipeline implementations have a fixed execution
management system.  However, such a system can provide more flexibility by
separating its execution management into a \keyterm{executive} object.  The
executive object is an independent object that manages pipeline execution.
Through polymorphism, different types of execution models can be
supported.  For example, VTK is designed as a demand-driven pipeline, but
with its interchangeable executives it can be converted to an event-driven
pipeline, as demonstrated by Vo \etal\lcite{Vo2010}.

Replacing the executive in a pipeline with centralized control is
straightforward.  The control is, by definition, its own separate unit.  In
contrast, a distributed control system must have an independent executive
object attached to each component in the pipeline.  The component objects
get relegated to only a function to execute whereas the executive manages
pipeline connections, data movement, and execution\lcite{VTKUsersGuide}.

\subsection{Out-of-Core Streaming}
\label{sec:OutOfCore}

An \keyterm{out-of-core} algorithm (or more formally an
\keyterm{external-memory algorithm}) is a general algorithmic technique
that can be applied when a data set is too large to fit within a computer's
internal memory.  When processing data out of core, only a fraction of the
data is read from storage at any one time\lcite{Vitter2001}.  The results
for that region of data are generated and stored, then the next segment of
data is read.

A rudimentary but effective way of performing out-of-core processing in a
visualization pipeline is to read data in pieces and let each piece flow
through the pipe independently.  Because pieces are fed into the pipeline
sequentially, this method of execution is often called \keyterm{streaming}.
In order for streaming to be feasible, the data must be separable (that is,
can be split into pieces), and the algorithm results must be invariant to
the number and order of the pieces (that is, each piece can be computed
independently).  In a demand-driven pipeline, it is also necessary to be
able to identify what piece of input is required to process some piece of
output\lcite{Law1999}.

Because the input data set is broken into pieces, the boundary between
pieces is important for many algorithms.  Boundaries are often handled by
adding an extra layer of cells, called \keyterm{ghost
  cells}\lcite{Ahrens2001}.  These ghost cells complete the neighborhood
information for each piece and can be removed from the final result.

Some algorithms can be run out-of-core with a simple execution model that
iterates over pieces.  However, most pipelines can implement streaming more
effectively with metadata, discussed in Section~\ref{sec:Metadata}.

\subsection{Block Iteration}
\label{sec:BlockIteration}

Some data sets are actually a conglomerate of smaller data sets.  These
smaller data sets are called either \keyterm{blocks} or \keyterm{domains}
of the whole.  One example of a multi-block data set is an assembly of
parts.  Another example is adaptive mesh refinement (AMR)\lcite{Berger1989}
in which a hierarchy of progressively finer grids selectively refines
regions of interest.

Many visualization algorithms can be applied independently to each block in
a multi-block data set.  Rather than have every component specifically
attend to the multi-block nature of the data, the execution management can
implicitly run an algorithm independently on every block in the data
set\lcite{VTKUsersGuide}.


\section{Metadata}
\label{sec:Metadata}

So far, we have considered the visualization pipeline as simply a flow
network for data, and the earliest implementations were just that.  Modern
visualization pipelines have introduced the concept of metadata, a brief
description of the actual data, into the pipeline.  The introduction of
metadata allows the pipeline to process data in more powerful ways.
Metadata can flow through the pipeline independent of, and often in
different directions than, the actual data.  The introduction of metadata
can in turn change the execution management of the pipeline.

\subsection{Regions}
\label{sec:Regions}

Perhaps the most important piece of information a visualization pipeline
can use is the region the data is defined over and the regions the data can
be split up into.  Knowing and specifying regions supports execution
management for out of core and parallel computation (described in Sections
\ref{sec:OutOfCore} and \ref{sec:ParallelExecution}, respectively).

Visualization pipelines operate on three basic types of regions.
\begin{description}
\item[Extents] are valid index ranges for regular multidimensional arrays
  of data.  Extents allow a fine granularity in defining regions as
  sub-arrays within a larger array.
\item[Pieces] are arbitrary collections of cells.  Pieces allow
  unstructured grids to be easily decomposed into discretionary regions.
\item[Blocks] (or domains) represent a logical domain decomposition.
  Blocks are similar to pieces in that they can represent arbitrary
  collections, but blocks are defined by the data set and their structures
  are considered to have some meaning.
\end{description}

Region metadata can flow throughout the pipeline independently of data.  A
general implementation to propagate region information and select regions
requires the three pipeline passes demonstrated in
Figure~\ref{fig:RegionPasses}\lcite{Ahrens2001}.

\begin{figure}[htbp]
  \centering
  \subfloat[Update Information]{Whole region moves down.\label{fig:RegionPasses:UpdateInformation}}
  \subfloat[Update Region]{Update region moves up.\label{fig:RegionPasses:UpdateRegion}}
  \subfloat[Update Data]{Data set moves down.\label{fig:RegionPasses:UpdateData}}
  \caption{The three pipeline passes for using regional data.}
  \label{fig:RegionPasses}
\end{figure}

In the first \keyterm{update information} pass, sources produce the entire
region they can generate, and that region gets passed down the pipeline.
As the region passes through filters, they have the opportunity to change
the region.  This could be because the filter is combining multiple regions
from multiple inputs.  It could also be because the filter is generating a
new topology, which has its own independent regions.

In the second \keyterm{update region} pass, the application decides what
region of data it would like a sink to process.  This update region is then
passed backward up the pipeline during which each filter transforms the
region respective of the output to a region respective of the input.  The
update region pass terminates at the sources, which receive the region of
data they must produce.

In the final \keyterm{update data} pass, the actual data flows through the
pipeline as described in Section~\ref{sec:ExecutionManagement}.

\subsection{Contracts}
\label{sec:Contracts}

impact, contract

\subsection{Time}
\label{sec:Time}

\subsection{Prioritized Streaming}
\label{sec:PrioritizedStreaming}

\subsection{Query Driven Visualization}
\label{sec:QueryDrivenVisualization}


\section{Parallel Execution}
\label{sec:ParallelExecution}

\subsection{Basic Parallel Execution Modes}
\label{sec:ParallelExecution:Modes}

\subsection{Rendering}
\label{sec:ParallelExecution:Rendering}

\subsection{Scalability}
\label{sec:Scalability}

\subsection{Hybrid Parallel}
\label{sec:HybridParallel}


\section{Miscellany}
\label{sec:Misc}

\subsection{Provenance}
\label{sec:Provenance}

\subsection{Scheduling}
\label{sec:Scheduling}

Multi-core.  Heterogeneous.  Hyperflow.

\subsection{In Situ}
\label{sec:InSitu}


\section{Visualization Pipeline Alternatives}
\label{sec:Alternatives}

\subsection{Field Abstraction}
\label{sec:FieldAbstraction}

Field Model (FM).  Field Encapsulation Library (FEL).

\subsection{MapReduce}
\label{sec:MapReduce}

\subsection{Worklets}
\label{sec:Worklets}


\section{Conclusion}
\label{sec:Conclusion}


\section{Acknowledgments}

This work was supported in part by the DOE Office of Science, Advanced
Scientific Computing Research, under award number 10-014707, program
manager Lucy Nowell.

Sandia National Laboratories is a multi-program laboratory operated by
Sandia Corporation, a wholly owned subsidiary of Lockheed Martin
Corporation, for the U.S. Department of Energy's National Nuclear Security
Administration.

\bibliographystyle{plain}
\bibliography{VisPipelines}

\end{document}
